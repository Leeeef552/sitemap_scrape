{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7fce07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total URL lines in directory: 1182\n",
      "Average error rate for each file: 9.380952380952381\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# Define your directory\n",
    "fil_dir = \"/home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/straits_times/unsuccessful\"\n",
    "\n",
    "# Use glob to find all .txt files in the directory\n",
    "txt_files = glob.glob(f\"{fil_dir}/*.jsonl\")\n",
    "\n",
    "# Initialize a counter\n",
    "total_lines = 0\n",
    "\n",
    "# Loop through each file and count lines\n",
    "for file_path in txt_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        total_lines += len(lines)\n",
    "\n",
    "print(f\"Total URL lines in directory: {total_lines}\")\n",
    "print(f\"Average error rate for each file: {total_lines/len(txt_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e18ebeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class CustomFormatter(logging.Formatter):\n",
    "    simple_fmt = '[%(asctime)s] [%(levelname)s] %(message)s'\n",
    "    error_fmt = '[%(asctime)s] [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s'\n",
    "\n",
    "    def format(self, record):\n",
    "        if record.levelno >= logging.ERROR:\n",
    "            self._style._fmt = self.error_fmt\n",
    "        else:\n",
    "            self._style._fmt = self.simple_fmt\n",
    "        return super().format(record)\n",
    "\n",
    "handler = logging.StreamHandler()\n",
    "handler.setFormatter(CustomFormatter(\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    "))\n",
    "logger = logging.getLogger(\"VLM WEBSCRAPING\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "239bf867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiofiles                 # NEW  ──────── async file I/O\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Any, Dict, List, Optional\n",
    "import asyncio, json, pathlib, traceback\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "from playwright.async_api import async_playwright, Browser, BrowserContext\n",
    "import random\n",
    "\n",
    "\n",
    "class AsyncScraper:\n",
    "    \"\"\"Scrapes articles using Playwright with batch processing and context reuse\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        llm_endpoint: str = \"http://localhost:8124/v1\", \n",
    "        model: str = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "        concurrency: int = 5\n",
    "    ):\n",
    "        self.llm_client = OpenAI(base_url=llm_endpoint, api_key=\"no-api-key-required\")\n",
    "        self.model = model\n",
    "        self.system_prompt = \"You are a news summarization assistant. Provide a concise 100-word or less summary of the article content. Focus on key facts, events, and conclusions. Respond with the summary directly without saying anything else.\"\n",
    "        self.concurrency = concurrency\n",
    "        self.semaphore = asyncio.Semaphore(concurrency)\n",
    "        self.browser: Optional[Browser] = None\n",
    "        self.contexts: List[BrowserContext] = []\n",
    "        self.context_semaphore = asyncio.Semaphore(concurrency)\n",
    "\n",
    "    async def __aenter__(self):\n",
    "        \"\"\"Async context manager entry\"\"\"\n",
    "        self.playwright = await async_playwright().start()\n",
    "        self.browser = await self.playwright.chromium.launch(\n",
    "            headless=True,\n",
    "            args=[\n",
    "                \"--no-sandbox\", \n",
    "                \"--disable-gpu\",\n",
    "                \"--disable-dev-shm-usage\",  # Reduce memory usage\n",
    "                \"--disable-web-security\",\n",
    "                \"--disable-features=VizDisplayCompositor\"\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        # Pre-create browser contexts for reuse\n",
    "        logger.info(f\"Creating {self.concurrency} browser contexts for reuse\")\n",
    "        for i in range(self.concurrency):\n",
    "            context = await self.browser.new_context(\n",
    "                viewport={\"width\": 1920, \"height\": 1080},\n",
    "                user_agent=(\n",
    "                    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                    \"Chrome/115.0.0.0 Safari/537.36\"\n",
    "                ),\n",
    "            )\n",
    "            self.contexts.append(context)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Async context manager exit\"\"\"\n",
    "        # Close all contexts\n",
    "        for context in self.contexts:\n",
    "            await context.close()\n",
    "        \n",
    "        if self.browser:\n",
    "            await self.browser.close()\n",
    "        if self.playwright:\n",
    "            await self.playwright.stop()\n",
    "\n",
    "    def _clean_content(self, article: Tag) -> str:\n",
    "        \"\"\"Extract and clean text content from article tag\"\"\"\n",
    "        # Remove unwanted elements\n",
    "        for element in article.find_all(['script', 'style', 'footer', 'nav', 'aside', 'header', 'button', 'form', 'input']):\n",
    "            element.decompose()\n",
    "\n",
    "        content = article.get_text(separator=\"\\n\\n\", strip=True)\n",
    "        if not content.strip():\n",
    "            logger.warning(\"No text content extracted after cleaning.\")\n",
    "        return content\n",
    "\n",
    "    async def _generate_summary(self, content: str) -> str:\n",
    "        try:\n",
    "            # Run the blocking LLM call in a separate thread\n",
    "            response = await asyncio.to_thread(\n",
    "                self.llm_client.chat.completions.create,\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": f\"{self.system_prompt}\\nArticle contents:\\n{content}\"}\n",
    "                ],\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM summary generation error: {e}\", exc_info=True)\n",
    "            return f\"Summary generation failed: {str(e)}\"\n",
    "        \n",
    "    def _extract_image_src(self, img_tag: Tag, page_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Resolve relative/absolute URLs for the given <img>.\n",
    "        \"\"\"\n",
    "        src = img_tag.get(\"src\") or img_tag.get(\"data-src\") or img_tag.get(\"data-original\")\n",
    "        return urljoin(page_url, src) if src else None\n",
    "\n",
    "    async def _get_available_context(self) -> BrowserContext:\n",
    "        \"\"\"Get an available browser context from the pool\"\"\"\n",
    "        async with self.context_semaphore:\n",
    "            # Simple round-robin selection\n",
    "            context_index = len(self.contexts) - self.context_semaphore._value - 1\n",
    "            return self.contexts[context_index % len(self.contexts)]\n",
    "\n",
    "    async def _fetch_page_content(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"Fetch page content using Playwright with context reuse\"\"\"\n",
    "        if not self.browser:\n",
    "            raise RuntimeError(\"Browser not started. Use 'async with'.\")\n",
    "        \n",
    "        async with self.semaphore:\n",
    "            await asyncio.sleep(random.uniform(0.1, 0.75))\n",
    "            context = await self._get_available_context()\n",
    "            page = await context.new_page()\n",
    "            try:\n",
    "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=15000)  # Reduced timeout\n",
    "                await page.wait_for_selector(\"article\", timeout=15000)\n",
    "                # Removed image navigation clicking - just get the current page content\n",
    "                \n",
    "                html = await page.content()\n",
    "                logger.debug(f\"Successfully fetched content from {url}\")\n",
    "                return BeautifulSoup(html, \"html.parser\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to fetch {url}: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                await page.close()  # Close the page but keep context alive\n",
    "\n",
    "    async def scrape_single_url(self, url: str) -> Dict[str, Any]:\n",
    "        \"\"\"Scrape a single URL for article content, metadata, images, and generate summary.\"\"\"\n",
    "        logger.debug(f\"Starting scrape for URL: {url}\")\n",
    "        \n",
    "        try:\n",
    "            max_retries = 2\n",
    "            for attempt in range(max_retries):\n",
    "                soup = await self._fetch_page_content(url)\n",
    "                if soup and soup.find(\"article\"):\n",
    "                    break\n",
    "                logger.info(f\"Retrying {url} (attempt {attempt+2}/{max_retries+1})\")\n",
    "                \n",
    "            article = soup.find(\"article\")\n",
    "            if not article:\n",
    "                logger.error(f\"No <article> tag found in {url}\")\n",
    "                raise RuntimeError(\"No <article> tag found\")\n",
    "\n",
    "            # Title\n",
    "            h1 = article.find(\"h1\")\n",
    "            title = (\n",
    "                h1.get_text(strip=True)\n",
    "                if h1\n",
    "                else (soup.title.string.strip() if soup.title else \"(untitled)\")\n",
    "            )\n",
    "\n",
    "            # Published date (rich logic)\n",
    "            pub_date: Optional[str] = None\n",
    "            time_tag = article.find(\"time\")\n",
    "            if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "                try:\n",
    "                    pub_date = dateparser.parse(time_tag[\"datetime\"]).isoformat()\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "            elif time_tag:\n",
    "                try:\n",
    "                    pub_date = dateparser.parse(time_tag.get_text(strip=True)).isoformat()\n",
    "                except (ValueError, TypeError):\n",
    "                    pass\n",
    "            else:\n",
    "                meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "                if meta and meta.has_attr(\"content\"):\n",
    "                    try:\n",
    "                        pub_date = dateparser.parse(meta[\"content\"]).isoformat()\n",
    "                    except (ValueError, TypeError):\n",
    "                        pass\n",
    "\n",
    "            # Extract and clean content\n",
    "            content = self._clean_content(article)\n",
    "\n",
    "            # Truncate if too long\n",
    "            truncated = False\n",
    "            if len(content) > 12000:\n",
    "                content = content[:12000]\n",
    "                truncated = True\n",
    "                logger.info(f\"Content for {url} was truncated to 12k characters\")\n",
    "\n",
    "            # Generate summary\n",
    "            summary = await self._generate_summary(content)\n",
    "            if summary.startswith(\"Summary generation failed:\"):\n",
    "                logger.error(f\"Summary generation failed for {url}: {summary}\")\n",
    "\n",
    "            # Collect images with alt text and caption\n",
    "            images: List[Dict[str, Any]] = []\n",
    "            \n",
    "            for picture in article.find_all(\"picture\"):\n",
    "                # grab alt text from the <img> if present\n",
    "                img_tag = picture.find(\"img\")\n",
    "                alt = img_tag.get(\"alt\", \"\").strip() or None if img_tag else None\n",
    "\n",
    "                # now pull every <source> and that <img>\n",
    "                for tag in picture.find_all([\"img\"]):\n",
    "                    src = self._extract_image_src(tag, url)\n",
    "                    if not src:\n",
    "                        continue\n",
    "                    images.append({\n",
    "                        \"image_url\": src,\n",
    "                        \"alt_text\": alt\n",
    "                    })\n",
    "\n",
    "            return {\n",
    "                \"article_url\": url,\n",
    "                \"site_title\": title,\n",
    "                \"publish_date\": pub_date,\n",
    "                \"summary\": summary,\n",
    "                \"truncated\": truncated,\n",
    "                \"images\": images,\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping {url}: {e}\", exc_info=True)\n",
    "            return (\"ERROR\", url, repr(e), traceback.format_exc())\n",
    "\n",
    "    async def scrape_urls_batch(self, urls: List[str]) -> List[Any]:\n",
    "        \"\"\"Scrape multiple URLs concurrently\"\"\"\n",
    "        logger.info(f\"Starting batch scrape of {len(urls)} URLs\")\n",
    "        \n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            await asyncio.sleep(0.15)   # 200 ms between task-spawns\n",
    "            tasks.append(self.scrape_single_url(url))\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        logger.info(f\"Completed batch scrape of {len(urls)} URLs\")\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93af792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 13:21:31] [INFO] Found 1 files to process\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cdc253232c49c9886ea31650b9f8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing urls:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-11 13:21:31] [ERROR] [759509774.py:122] Error processing /home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/business_times/test/bt_2013_09.txt: asyncio.run() cannot be called from a running event loop\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3657239/759509774.py\", line 113, in main\n",
      "    result = asyncio.run(process_txt_async(txt_file, OUT_DIR, ERR_DIR, CONCURRENCY))\n",
      "  File \"/usr/lib/python3.10/asyncio/runners.py\", line 33, in run\n",
      "    raise RuntimeError(\n",
      "RuntimeError: asyncio.run() cannot be called from a running event loop\n",
      "/tmp/ipykernel_3657239/759509774.py:122: RuntimeWarning: coroutine 'process_txt_async' was never awaited\n",
      "  logger.error(f\"Error processing {txt_file}: {e}\", exc_info=True)\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2025-07-11 13:21:31] [INFO] All files processed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time taken 0.011708259582519531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "async def process_txt_async(\n",
    "    txt_path: pathlib.Path,\n",
    "    out_dir: pathlib.Path,\n",
    "    err_dir: pathlib.Path,\n",
    "    concurrency: int = 5,\n",
    ") -> str | None:\n",
    "    year_month = txt_path.stem\n",
    "    out_file = out_dir / f\"{year_month}.jsonl\"\n",
    "    err_file = err_dir / f\"{year_month}_errors.jsonl\"\n",
    "\n",
    "    logger.info(f\"Processing {txt_path.name}…\")\n",
    "\n",
    "    # ── 1) Read all URLs from the .txt ────────────────────────────────────────\n",
    "    try:\n",
    "        urls = [ln.strip() for ln in txt_path.read_text().splitlines() if ln.strip()]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error reading {txt_path}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "    if not urls:\n",
    "        logger.warning(f\"No URLs found in {txt_path.name}\")\n",
    "        return year_month\n",
    "\n",
    "    # ── 2) Filter out URLs we've already scraped successfully ───────────────\n",
    "    processed_urls = set()\n",
    "    if out_file.exists():\n",
    "        for line in out_file.read_text(encoding=\"utf-8\").splitlines():\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                if isinstance(rec, dict) and \"article_url\" in rec:\n",
    "                    processed_urls.add(rec[\"article_url\"])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "\n",
    "    # Keep only the ones not yet done\n",
    "    urls = [u for u in urls if u not in processed_urls]\n",
    "    if not urls:\n",
    "        logger.info(f\"All URLs in {txt_path.name} are already processed; skipping.\")\n",
    "        return year_month\n",
    "\n",
    "    # ── 3) Now urls contains only new entries; proceed as before ───────────\n",
    "    success_count = error_count = 0\n",
    "\n",
    "    async with AsyncScraper(concurrency=concurrency) as scraper, \\\n",
    "               aiofiles.open(out_file, \"a\", encoding=\"utf-8\") as ok_f, \\\n",
    "               aiofiles.open(err_file, \"a\", encoding=\"utf-8\") as er_f:\n",
    "\n",
    "        # Kick off all scrapes\n",
    "        scrape_tasks = [asyncio.create_task(scraper.scrape_single_url(u)) for u in urls]\n",
    "\n",
    "        # Wrap the completion iterator in tqdm\n",
    "        with tqdm(\n",
    "            total=len(scrape_tasks),\n",
    "            desc=f\"Scraping URLs in {txt_path.name}\",\n",
    "            unit=\"url\",\n",
    "            leave=False\n",
    "        ) as pbar:\n",
    "            for coro in asyncio.as_completed(scrape_tasks):\n",
    "                item = await coro\n",
    "\n",
    "                # Determine which file to write to\n",
    "                is_error = isinstance(item, tuple) and item and item[0] == \"ERROR\"\n",
    "                target_f = er_f if is_error else ok_f\n",
    "\n",
    "                # Write & flush\n",
    "                await target_f.write(json.dumps(item, default=str) + \"\\n\")\n",
    "                await target_f.flush()\n",
    "\n",
    "                # Logging and counters\n",
    "                if is_error:\n",
    "                    error_count += 1\n",
    "                    _, bad_url, msg, _tb = item\n",
    "                    logger.error(f\"Error scraping {bad_url}: {msg}\")\n",
    "                else:\n",
    "                    success_count += 1\n",
    "                    logger.debug(f\"Saved {item['article_url']}\")\n",
    "\n",
    "                # Advance the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "    logger.info(\n",
    "        f\"Completed {txt_path.name}: {success_count} ok, {error_count} errors\"\n",
    "    )\n",
    "    return year_month\n",
    "\n",
    "\n",
    "def main():\n",
    "    BASE_DIR = pathlib.Path(\"/home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/business_times\")\n",
    "    UNSEEN_DIR = BASE_DIR / \"test\"  # Original .txt files here\n",
    "    SEEN_DIR = BASE_DIR / \"seen\"      # Processed .txt files moved here\n",
    "    OUT_DIR = BASE_DIR / \"scraped\"\n",
    "    ERR_DIR = BASE_DIR / \"unsuccessful\"\n",
    "\n",
    "    # Ensure directories exist\n",
    "    UNSEEN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    SEEN_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    ERR_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    CONCURRENCY = 5  # Increased from 5 - URLs processed concurrently per file\n",
    "\n",
    "    txt_files = list(UNSEEN_DIR.glob(\"*.txt\"))\n",
    "\n",
    "    if not txt_files:\n",
    "        logger.warning(f\"No .txt files found in {UNSEEN_DIR}\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Found {len(txt_files)} files to process\")\n",
    "\n",
    "    # Process files one by one (you can modify this to process multiple files concurrently if needed)\n",
    "    for txt_file in tqdm(txt_files, desc=\"Processing urls\"):\n",
    "        try:\n",
    "            result = asyncio.run(process_txt_async(txt_file, OUT_DIR, ERR_DIR, CONCURRENCY))\n",
    "            if result:\n",
    "                logger.info(f\"Processed {txt_file.name} with result {result}\")\n",
    "            \n",
    "            # Move processed file to seen directory\n",
    "            seen_path = SEEN_DIR / txt_file.name\n",
    "            txt_file.rename(seen_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {txt_file}: {e}\", exc_info=True)\n",
    "\n",
    "    logger.info(\"All files processed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start = time.time()\n",
    "    main()\n",
    "    end = time.time()\n",
    "    print(f\"total time taken {end-start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
