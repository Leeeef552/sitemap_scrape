{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5441dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'site_title': \"Budget 2015: Beware the 'trust fund kids' mindset\", 'publish_date': '2015-03-01T06:26:30+08:00', 'image_url': 'https://static1.straitstimes.com.sg/s3fs-public/articles/2015/03/01/ST_20150301_ADAMMH_1105159e_2x.jpg?VersionId=ZYSN5X8rwPeQjCfAlpTesfO3kgyjfAym', 'alt_text': '-- ST ILLUSTRATION: ADAM LEE', 'caption': '-- ST ILLUSTRATION: ADAM LEE'}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    \"\"\"\n",
    "    Scrapes a page for <article> → images + captions.\n",
    "\n",
    "    Each image is returned as its own JSON object with keys:\n",
    "        site_title, publish_date, image_url, alt_text, caption\n",
    "    \"\"\"\n",
    "\n",
    "    def _extract_image_src(self, img_tag: Tag, page_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Resolve relative/absolute URLs for the given <img>.\n",
    "        \"\"\"\n",
    "        src = img_tag.get(\"src\") or img_tag.get(\"data-src\") or img_tag.get(\"data-original\")\n",
    "        return urljoin(page_url, src) if src else None\n",
    "\n",
    "    def _extract_caption(self, img_tag: Tag) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Return the best-guess caption for `img_tag`.\n",
    "\n",
    "        Search order (stop at the first non-empty hit):\n",
    "        1. <figure><figcaption>\n",
    "        2. Immediate next / previous sibling that is <figcaption>, <p>, or <span>\n",
    "        3. Any ancestor with class like 'caption' or 'credit'\n",
    "        4. Up to three next *or* previous block-level siblings (<p>, <div>, <figcaption>, <span>)\n",
    "        5. Fallback: non-empty alt text\n",
    "        \"\"\"\n",
    "        # 1. Ideal case: <figure> → <figcaption>\n",
    "        fig = img_tag.find_parent(\"figure\")\n",
    "        if fig:\n",
    "            figcap = fig.find(\"figcaption\")\n",
    "            if figcap:\n",
    "                text = figcap.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 2. Immediate siblings (<figcaption>, <p>, <span>)\n",
    "        for sib in (img_tag.find_next_sibling(), img_tag.find_previous_sibling()):\n",
    "            if sib and isinstance(sib, Tag) and sib.name in {\"figcaption\", \"p\", \"span\"}:\n",
    "                text = sib.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 3. Any ancestor with caption-like class\n",
    "        parent = img_tag.parent\n",
    "        while parent and parent.name not in {\"article\", \"body\"}:\n",
    "            classes = parent.get(\"class\", [])\n",
    "            if any(re.search(r\"(caption|credit)\", c, re.I) for c in classes):\n",
    "                text = parent.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "            parent = parent.parent\n",
    "\n",
    "        # 4. Up to 3 forward/back block-level siblings\n",
    "        for direction in (\"next\", \"previous\"):\n",
    "            sib_iter = (\n",
    "                img_tag.next_siblings if direction == \"next\" else img_tag.previous_siblings\n",
    "            )\n",
    "            count = 0\n",
    "            for sib in sib_iter:\n",
    "                if isinstance(sib, Tag) and sib.name in {\"p\", \"div\", \"figcaption\", \"span\"}:\n",
    "                    text = sib.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        return text\n",
    "                    count += 1\n",
    "                    if count == 3:  # stop after 3 hops\n",
    "                        break\n",
    "\n",
    "        # 5. Last resort: alt text\n",
    "        alt = img_tag.get(\"alt\", \"\").strip()\n",
    "        return alt or None\n",
    "\n",
    "\n",
    "    def scrape(self, url: str) -> List[Dict[str, Any]]:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "        article = soup.find(\"article\")\n",
    "        if not article:\n",
    "            raise RuntimeError(\"No <article> tag found\")\n",
    "\n",
    "        # Title\n",
    "        h1 = article.find(\"h1\")\n",
    "        title = (\n",
    "            h1.get_text(strip=True)\n",
    "            if h1\n",
    "            else (soup.title.string.strip() if soup.title else \"(untitled)\")\n",
    "        )\n",
    "\n",
    "        # Published date\n",
    "        pub_date: Optional[str] = None\n",
    "        time_tag = article.find(\"time\")\n",
    "        if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "            pub_date = dateparser.parse(time_tag[\"datetime\"]).isoformat()\n",
    "        elif time_tag:\n",
    "            pub_date = dateparser.parse(time_tag.get_text(strip=True)).isoformat()\n",
    "        else:\n",
    "            meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "            if meta and meta.has_attr(\"content\"):\n",
    "                pub_date = dateparser.parse(meta[\"content\"]).isoformat()\n",
    "\n",
    "        # Collect images\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for img in article.find_all(\"img\"):\n",
    "            src = self._extract_image_src(img, url)\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"site_title\": title,\n",
    "                    \"publish_date\": pub_date,\n",
    "                    \"image_url\": src,\n",
    "                    \"alt_text\": img.get(\"alt\", \"\").strip() or None,\n",
    "                    \"caption\": self._extract_caption(img),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ArticleScraper()\n",
    "    images = scraper.scrape(\n",
    "        \"https://www.straitstimes.com/opinion/budget-2015-beware-the-trust-fund-kids-mindset\"\n",
    "    )\n",
    "    for img in images:\n",
    "        print(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "532403ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total URLs: 4790\n",
      "First : https://www.straitstimes.com/business/thai-based-airlines-in-for-turbulence-after-bans-on-charter-flights\n",
      "Last  : https://www.straitstimes.com/lifestyle/mid-life-reflections-on-loving-life\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from bs4 import BeautifulSoup   # pip install beautifulsoup4 lxml\n",
    "\n",
    "def fetch_sitemap_bs4(year: int, month: int, *, timeout=15) -> list[str]:\n",
    "    \"\"\"\n",
    "    Pull <loc> links from https://www.straitstimes.com/sitemap/{year}/{month}/feeds.xml\n",
    "    (month must be two-digit: 01–12).\n",
    "    \"\"\"\n",
    "    url = f\"https://www.straitstimes.com/sitemap/{year}/{month:02d}/feeds.xml\"\n",
    "    r = requests.get(url, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(r.content, \"xml\")   # 'xml' parser ≈ lxml-xml fallback to xml.etree\n",
    "    return [tag.get_text(strip=True) for tag in soup.find_all(\"loc\")]\n",
    "\n",
    "def save_urls(urls: list[str], outfile: str | Path):\n",
    "    Path(outfile).write_text(\"\\n\".join(urls), encoding=\"utf-8\")\n",
    "\n",
    "# ─── example ─────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    urls = fetch_sitemap_bs4(2015, 3)          # ⚠️ avoid CURRENT month (incomplete)\n",
    "    print(f\"\\nTotal URLs: {len(urls)}\")\n",
    "    print(\"First :\", urls[0])\n",
    "    print(\"Last  :\", urls[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import asyncio\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List\n",
    "\n",
    "import httpx                     # pip install httpx\n",
    "from bs4 import BeautifulSoup     # pip install beautifulsoup4 lxml\n",
    "\n",
    "MONTH_FEED_RE = re.compile(r\"/(\\d{4})/(\\d{2})/feeds\\.xml$\")   # keep only YYYY/MM feeds\n",
    "\n",
    "@dataclass\n",
    "class LinkExtractor:\n",
    "    index_url: str\n",
    "    out_dir: Path | str = \"/home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/straitsTimes/st_sitemaps\"\n",
    "    timeout: float = 15.0\n",
    "    polite_delay: float = 1.0\n",
    "    max_concurrency: int = 5\n",
    "\n",
    "    # ────────────────────────── public helpers ──────────────────────────────\n",
    "    async def dump_async(self) -> None:\n",
    "        \"\"\"Asynchronously download every past month and save to .txt files.\"\"\"\n",
    "        self.out_dir = Path(self.out_dir)\n",
    "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
    "            month_feeds = await self._sitemap_links(client)\n",
    "\n",
    "            if not month_feeds:\n",
    "                print(\"No month feeds found (or all filtered out).\")\n",
    "                return\n",
    "\n",
    "            sem = asyncio.Semaphore(self.max_concurrency)\n",
    "            tasks = [\n",
    "                asyncio.create_task(self._process_month(feed_url, client, sem))\n",
    "                for feed_url in month_feeds\n",
    "            ]\n",
    "            await asyncio.gather(*tasks)\n",
    "\n",
    "    def dump(self):\n",
    "        try:\n",
    "            loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:           # no loop → we're in a vanilla script\n",
    "            loop = None\n",
    "\n",
    "        if loop and loop.is_running():\n",
    "            # notebook / web-server context → create and return a Task\n",
    "            return asyncio.create_task(self.dump_async())\n",
    "        else:\n",
    "            # classic script → safe to spin up a fresh loop\n",
    "            asyncio.run(self.dump_async())\n",
    "\n",
    "    # ────────────────────────── internals ───────────────────────────────────\n",
    "    async def _sitemap_links(self, client: httpx.AsyncClient) -> List[str]:\n",
    "        \"\"\"Return monthly feeds, filtering out current month & sections.xml.\"\"\"\n",
    "        r = await client.get(self.index_url)\n",
    "        r.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(r.content, \"xml\")\n",
    "        raw_links = [\n",
    "            loc.get_text(strip=True)\n",
    "            for loc in soup.find_all(\"loc\")\n",
    "            if loc.parent.name == \"sitemap\"\n",
    "        ]\n",
    "\n",
    "        # figure out YYYY/MM for 'today' (Singapore time is irrelevant for month test)\n",
    "        y_now, m_now = datetime.now(timezone.utc).year, datetime.now(timezone.utc).month\n",
    "\n",
    "        feeds: list[str] = []\n",
    "        for link in raw_links:\n",
    "            m = MONTH_FEED_RE.search(link)\n",
    "            if not m:                          # skips sections.xml & anything odd\n",
    "                continue\n",
    "            yr, mo = int(m.group(1)), int(m.group(2))\n",
    "            if (yr, mo) == (y_now, m_now):     # skip current month\n",
    "                continue\n",
    "            feeds.append(link)\n",
    "\n",
    "        return feeds\n",
    "\n",
    "    async def _month_urls(\n",
    "        self, feed_url: str, client: httpx.AsyncClient\n",
    "    ) -> Iterable[str]:\n",
    "        \"\"\"Return every <loc> article URL from a single feeds.xml.\"\"\"\n",
    "        r = await client.get(feed_url)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.content, \"xml\")\n",
    "        return (loc.get_text(strip=True) for loc in soup.find_all(\"loc\"))\n",
    "\n",
    "    async def _process_month(\n",
    "        self,\n",
    "        feed_url: str,\n",
    "        client: httpx.AsyncClient,\n",
    "        sem: asyncio.Semaphore,\n",
    "    ) -> None:\n",
    "        \"\"\"Download one month feed, write out its TXT file.\"\"\"\n",
    "        async with sem:                 # limit concurrent requests\n",
    "            try:\n",
    "                urls = list(await self._month_urls(feed_url, client))\n",
    "            except httpx.HTTPError as e:\n",
    "                print(\"   ERR ·\", feed_url, \"→\", e)\n",
    "                return\n",
    "\n",
    "            if not urls:\n",
    "                print(\"   0   · (empty) ·\", feed_url)\n",
    "                return\n",
    "\n",
    "            # derive filename st_YYYY_MM.txt from the URL\n",
    "            m = MONTH_FEED_RE.search(feed_url)\n",
    "            fname = f\"st_{m.group(1)}_{m.group(2)}.txt\"\n",
    "            outpath = self.out_dir / fname\n",
    "            outpath.write_text(\"\\n\".join(urls), encoding=\"utf-8\")\n",
    "\n",
    "            print(f\"{len(urls):5d} · {outpath.relative_to(self.out_dir)}\")\n",
    "            await asyncio.sleep(self.polite_delay)  # respectful crawl pace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e491d367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Task pending name='Task-5' coro=<LinkExtractor.dump_async() running at /tmp/ipykernel_1672776/444078790.py:25>>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5000 · st_2025_05.txt\n",
      " 5000 · st_2025_01.txt\n",
      " 5000 · st_2024_11.txt\n",
      " 5000 · st_2025_06.txt\n",
      " 5000 · st_2025_02.txt\n",
      " 5000 · st_2025_04.txt\n",
      " 5000 · st_2025_03.txt\n",
      " 5000 · st_2024_12.txt\n",
      " 5000 · st_2024_10.txt\n",
      " 5000 · st_2024_07.txt\n",
      " 5000 · st_2024_06.txt\n",
      " 5000 · st_2024_05.txt\n",
      " 5000 · st_2024_09.txt\n",
      " 5000 · st_2024_08.txt\n",
      " 5000 · st_2024_04.txt\n",
      " 5000 · st_2024_03.txt\n",
      " 5000 · st_2024_01.txt\n",
      " 5000 · st_2024_02.txt\n",
      " 4975 · st_2023_08.txt\n",
      " 4051 · st_2023_06.txt\n",
      " 4260 · st_2023_07.txt\n",
      " 4219 · st_2023_05.txt\n",
      " 4997 · st_2023_12.txt\n",
      " 4990 · st_2023_11.txt\n",
      " 4961 · st_2023_09.txt\n",
      " 3965 · st_2023_04.txt\n",
      " 4394 · st_2023_01.txt\n",
      " 4608 · st_2022_11.txt\n",
      " 4608 · st_2023_03.txt\n",
      " 4968 · st_2023_10.txt\n",
      " 4214 · st_2023_02.txt\n",
      " 1930 · st_2022_10.txt\n",
      " 4221 · st_2022_12.txt\n",
      " 5000 · st_2022_04.txt\n",
      " 3319 · st_2022_09.txt\n",
      " 5000 · st_2022_08.txt\n",
      " 5000 · st_2022_07.txt\n",
      " 5000 · st_2022_06.txt\n",
      " 5000 · st_2022_05.txt\n",
      " 5000 · st_2022_02.txt\n",
      " 5000 · st_2022_03.txt\n",
      " 5000 · st_2022_01.txt\n",
      " 5000 · st_2021_11.txt\n",
      " 5000 · st_2021_12.txt\n",
      " 5000 · st_2021_10.txt\n",
      " 5000 · st_2021_09.txt\n",
      " 5000 · st_2021_08.txt\n",
      " 5000 · st_2021_06.txt\n",
      " 5000 · st_2021_07.txt\n",
      " 5000 · st_2021_05.txt\n",
      " 5000 · st_2020_10.txt\n",
      " 5000 · st_2020_11.txt\n",
      " 5000 · st_2021_03.txt\n",
      " 5000 · st_2021_02.txt\n",
      " 5000 · st_2021_01.txt\n",
      " 5000 · st_2021_04.txt\n",
      " 5000 · st_2020_12.txt\n",
      " 5000 · st_2020_07.txt\n",
      " 5000 · st_2020_05.txt\n",
      " 5000 · st_2020_01.txt\n",
      " 5000 · st_2020_09.txt\n",
      " 4094 · st_2019_12.txt\n",
      " 4760 · st_2019_10.txt\n",
      " 5000 · st_2020_08.txt\n",
      " 5000 · st_2020_06.txt\n",
      " 5000 · st_2020_04.txt\n",
      " 5000 · st_2020_03.txt\n",
      " 5000 · st_2020_02.txt\n",
      " 4601 · st_2019_11.txt\n",
      " 4761 · st_2019_07.txt\n",
      " 4861 · st_2019_09.txt\n",
      " 4563 · st_2019_05.txt\n",
      " 4576 · st_2019_04.txt\n",
      " 4429 · st_2019_08.txt\n",
      " 4488 · st_2019_02.txt\n",
      " 4352 · st_2019_06.txt\n",
      " 4934 · st_2018_07.txt\n",
      " 5000 · st_2019_03.txt\n",
      " 3880 · st_2018_12.txt\n",
      " 4701 · st_2019_01.txt\n",
      " 4319 · st_2018_11.txt\n",
      " 4285 · st_2018_10.txt\n",
      " 3970 · st_2018_09.txt\n",
      " 4251 · st_2018_08.txt\n",
      " 4444 · st_2018_06.txt\n",
      " 4708 · st_2018_05.txt\n",
      " 4992 · st_2018_04.txt\n",
      " 3975 · st_2017_12.txt\n",
      " 4610 · st_2018_02.txt\n",
      " 5000 · st_2018_03.txt\n",
      " 5000 · st_2018_01.txt\n",
      " 4516 · st_2017_11.txt\n",
      " 4641 · st_2017_10.txt\n",
      " 4339 · st_2017_09.txt\n",
      " 3899 · st_2017_05.txt\n",
      " 3777 · st_2017_03.txt\n",
      " 4204 · st_2017_06.txt\n",
      " 3717 · st_2017_04.txt\n",
      " 4748 · st_2017_08.txt\n",
      " 3317 · st_2017_02.txt\n",
      " 3574 · st_2017_01.txt\n",
      " 4226 · st_2016_12.txt\n",
      " 4753 · st_2016_10.txt\n",
      " 4912 · st_2017_07.txt\n",
      " 4531 · st_2016_11.txt\n",
      " 4448 · st_2016_09.txt\n",
      " 4871 · st_2016_08.txt\n",
      " 4393 · st_2016_07.txt\n",
      " 4609 · st_2016_06.txt\n",
      " 4621 · st_2016_05.txt\n",
      " 4572 · st_2016_03.txt\n",
      " 4451 · st_2016_04.txt\n",
      " 4338 · st_2016_02.txt\n",
      " 4506 · st_2015_12.txt\n",
      " 4664 · st_2016_01.txt\n",
      " 4691 · st_2015_11.txt\n",
      " 4858 · st_2015_10.txt\n",
      " 4352 · st_2015_09.txt\n",
      " 4875 · st_2015_08.txt\n",
      " 4846 · st_2015_07.txt\n",
      " 4240 · st_2015_06.txt\n",
      " 4516 · st_2015_05.txt\n",
      " 4091 · st_2015_02.txt\n",
      " 4190 · st_2015_04.txt\n",
      " 4485 · st_2015_01.txt\n",
      " 4790 · st_2015_03.txt\n"
     ]
    }
   ],
   "source": [
    "extractor = LinkExtractor(\n",
    "    index_url=\"https://www.straitstimes.com/sitemap.xml\",\n",
    "    timeout=10,            # adjust as desired\n",
    "    polite_delay=0.5,      # half-second between requests\n",
    "    max_concurrency=8,     # higher = faster, but stay polite!\n",
    ")\n",
    "extractor.dump()           # sync call; runs an async loop under the hood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fce07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
