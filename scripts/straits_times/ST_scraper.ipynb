{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d09a234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34510d",
   "metadata": {},
   "source": [
    "### Class to scrape the XML links and to collate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaee754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import annotations\n",
    "\n",
    "# import asyncio\n",
    "# import re\n",
    "# from dataclasses import dataclass\n",
    "# from datetime import datetime, timezone\n",
    "# from pathlib import Path\n",
    "# from typing import Iterable, List\n",
    "\n",
    "# import httpx                     # pip install httpx\n",
    "# from bs4 import BeautifulSoup     # pip install beautifulsoup4 lxml\n",
    "\n",
    "# MONTH_FEED_RE = re.compile(r\"/(\\d{4})/(\\d{2})/feeds\\.xml$\")   # keep only YYYY/MM feeds\n",
    "\n",
    "# @dataclass\n",
    "# class LinkExtractor:\n",
    "#     index_url: str\n",
    "#     out_dir: Path | str = \"/home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/straitsTimes/st_sitemaps\"\n",
    "#     timeout: float = 15.0\n",
    "#     polite_delay: float = 1.0\n",
    "#     max_concurrency: int = 5\n",
    "\n",
    "#     # ────────────────────────── public helpers ──────────────────────────────\n",
    "#     async def dump_async(self) -> None:\n",
    "#         \"\"\"Asynchronously download every past month and save to .txt files.\"\"\"\n",
    "#         self.out_dir = Path(self.out_dir)\n",
    "#         self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
    "#             month_feeds = await self._sitemap_links(client)\n",
    "\n",
    "#             if not month_feeds:\n",
    "#                 print(\"No month feeds found (or all filtered out).\")\n",
    "#                 return\n",
    "\n",
    "#             sem = asyncio.Semaphore(self.max_concurrency)\n",
    "#             tasks = [\n",
    "#                 asyncio.create_task(self._process_month(feed_url, client, sem))\n",
    "#                 for feed_url in month_feeds\n",
    "#             ]\n",
    "#             await asyncio.gather(*tasks)\n",
    "\n",
    "#     def dump(self):\n",
    "#         try:\n",
    "#             loop = asyncio.get_running_loop()\n",
    "#         except RuntimeError:           # no loop → we're in a vanilla script\n",
    "#             loop = None\n",
    "\n",
    "#         if loop and loop.is_running():\n",
    "#             # notebook / web-server context → create and return a Task\n",
    "#             return asyncio.create_task(self.dump_async())\n",
    "#         else:\n",
    "#             # classic script → safe to spin up a fresh loop\n",
    "#             asyncio.run(self.dump_async())\n",
    "\n",
    "#     # ────────────────────────── internals ───────────────────────────────────\n",
    "#     async def _sitemap_links(self, client: httpx.AsyncClient) -> List[str]:\n",
    "#         \"\"\"Return monthly feeds, filtering out current month & sections.xml.\"\"\"\n",
    "#         r = await client.get(self.index_url)\n",
    "#         r.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(r.content, \"xml\")\n",
    "#         raw_links = [\n",
    "#             loc.get_text(strip=True)\n",
    "#             for loc in soup.find_all(\"loc\")\n",
    "#             if loc.parent.name == \"sitemap\"\n",
    "#         ]\n",
    "\n",
    "#         # figure out YYYY/MM for 'today' (Singapore time is irrelevant for month test)\n",
    "#         y_now, m_now = datetime.now(timezone.utc).year, datetime.now(timezone.utc).month\n",
    "\n",
    "#         feeds: list[str] = []\n",
    "#         for link in raw_links:\n",
    "#             m = MONTH_FEED_RE.search(link)\n",
    "#             if not m:                          # skips sections.xml & anything odd\n",
    "#                 continue\n",
    "#             yr, mo = int(m.group(1)), int(m.group(2))\n",
    "#             if (yr, mo) == (y_now, m_now):     # skip current month\n",
    "#                 continue\n",
    "#             feeds.append(link)\n",
    "\n",
    "#         return feeds\n",
    "\n",
    "#     async def _month_urls(\n",
    "#         self, feed_url: str, client: httpx.AsyncClient\n",
    "#     ) -> Iterable[str]:\n",
    "#         \"\"\"Return every <loc> article URL from a single feeds.xml.\"\"\"\n",
    "#         r = await client.get(feed_url)\n",
    "#         r.raise_for_status()\n",
    "#         soup = BeautifulSoup(r.content, \"xml\")\n",
    "#         return (loc.get_text(strip=True) for loc in soup.find_all(\"loc\"))\n",
    "\n",
    "#     async def _process_month(\n",
    "#         self,\n",
    "#         feed_url: str,\n",
    "#         client: httpx.AsyncClient,\n",
    "#         sem: asyncio.Semaphore,\n",
    "#     ) -> None:\n",
    "#         \"\"\"Download one month feed, write out its TXT file.\"\"\"\n",
    "#         async with sem:                 # limit concurrent requests\n",
    "#             try:\n",
    "#                 urls = list(await self._month_urls(feed_url, client))\n",
    "#             except httpx.HTTPError as e:\n",
    "#                 print(\"   ERR ·\", feed_url, \"→\", e)\n",
    "#                 return\n",
    "\n",
    "#             if not urls:\n",
    "#                 print(\"   0   · (empty) ·\", feed_url)\n",
    "#                 return\n",
    "\n",
    "#             # derive filename st_YYYY_MM.txt from the URL\n",
    "#             m = MONTH_FEED_RE.search(feed_url)\n",
    "#             fname = f\"st_{m.group(1)}_{m.group(2)}.txt\"\n",
    "#             outpath = self.out_dir / fname\n",
    "#             outpath.write_text(\"\\n\".join(urls), encoding=\"utf-8\")\n",
    "\n",
    "#             print(f\"{len(urls):5d} · {outpath.relative_to(self.out_dir)}\")\n",
    "#             await asyncio.sleep(self.polite_delay)  # respectful crawl pace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor = LinkExtractor(\n",
    "#     index_url=\"https://www.straitstimes.com/sitemap.xml\",\n",
    "#     timeout=10,            # adjust as desired\n",
    "#     polite_delay=0.5,      # half-second between requests\n",
    "#     max_concurrency=8,     # higher = faster, but stay polite!\n",
    "# )\n",
    "# extractor.dump()           # sync call; runs an async loop under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3c86b",
   "metadata": {},
   "source": [
    "### Class to scrape 1 singular article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    #   Scrapes a page for <article> → images + captions.\n",
    "    #   Each image is returned as its own JSON object with keys: site_title, publish_date, image_url, alt_text, caption\n",
    "\n",
    "    def _extract_image_src(self, img_tag: Tag, page_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Resolve relative/absolute URLs for the given <img>.\n",
    "        \"\"\"\n",
    "        src = img_tag.get(\"src\") or img_tag.get(\"data-src\") or img_tag.get(\"data-original\")\n",
    "        return urljoin(page_url, src) if src else None\n",
    "\n",
    "    def _extract_caption(self, img_tag: Tag) -> Optional[str]:\n",
    "        # Return the best-guess caption for `img_tag`.\n",
    "\n",
    "        # 1. Ideal case: <figure> → <figcaption>\n",
    "        fig = img_tag.find_parent(\"figure\")\n",
    "        if fig:\n",
    "            figcap = fig.find(\"figcaption\")\n",
    "            if figcap:\n",
    "                text = figcap.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 2. Immediate siblings (<figcaption>, <p>, <span>)\n",
    "        for sib in (img_tag.find_next_sibling(), img_tag.find_previous_sibling()):\n",
    "            if sib and isinstance(sib, Tag) and sib.name in {\"figcaption\", \"p\", \"span\"}:\n",
    "                text = sib.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 3. Any ancestor with caption-like class\n",
    "        parent = img_tag.parent\n",
    "        while parent and parent.name not in {\"article\", \"body\"}:\n",
    "            classes = parent.get(\"class\", [])\n",
    "            if any(re.search(r\"(caption|credit)\", c, re.I) for c in classes):\n",
    "                text = parent.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "            parent = parent.parent\n",
    "\n",
    "        # 4. Up to 3 forward/back block-level siblings\n",
    "        for direction in (\"next\", \"previous\"):\n",
    "            sib_iter = (\n",
    "                img_tag.next_siblings if direction == \"next\" else img_tag.previous_siblings\n",
    "            )\n",
    "            count = 0\n",
    "            for sib in sib_iter:\n",
    "                if isinstance(sib, Tag) and sib.name in {\"p\", \"div\", \"figcaption\", \"span\"}:\n",
    "                    text = sib.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        return text\n",
    "                    count += 1\n",
    "                    if count == 3:  # stop after 3 hops\n",
    "                        break\n",
    "\n",
    "        # 5. Last resort: alt text\n",
    "        alt = img_tag.get(\"alt\", \"\").strip()\n",
    "        return alt or None\n",
    "\n",
    "\n",
    "    def scrape(self, url: str) -> List[Dict[str, Any]]:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "        article = soup.find(\"article\")\n",
    "        if not article:\n",
    "            raise RuntimeError(\"No <article> tag found\")\n",
    "\n",
    "        # Title\n",
    "        h1 = article.find(\"h1\")\n",
    "        title = (\n",
    "            h1.get_text(strip=True)\n",
    "            if h1\n",
    "            else (soup.title.string.strip() if soup.title else \"(untitled)\")\n",
    "        )\n",
    "\n",
    "        # Published date\n",
    "        pub_date: Optional[str] = None\n",
    "        time_tag = article.find(\"time\")\n",
    "        if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "            pub_date = dateparser.parse(time_tag[\"datetime\"]).isoformat()\n",
    "        elif time_tag:\n",
    "            pub_date = dateparser.parse(time_tag.get_text(strip=True)).isoformat()\n",
    "        else:\n",
    "            meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "            if meta and meta.has_attr(\"content\"):\n",
    "                pub_date = dateparser.parse(meta[\"content\"]).isoformat()\n",
    "\n",
    "        # Collect images\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for img in article.find_all(\"img\"):\n",
    "            src = self._extract_image_src(img, url)\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"site_title\": title,\n",
    "                    \"publish_date\": pub_date,\n",
    "                    \"image_url\": src,\n",
    "                    \"alt_text\": img.get(\"alt\", \"\").strip() or None,\n",
    "                    \"caption\": self._extract_caption(img),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ff484",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = ArticleScraper()\n",
    "images = scraper.scrape(\n",
    "    \"https://www.straitstimes.com/opinion/budget-2015-beware-the-trust-fund-kids-mindset\"\n",
    ")\n",
    "for img in images:\n",
    "    print(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
