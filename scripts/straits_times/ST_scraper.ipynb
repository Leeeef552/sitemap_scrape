{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from urllib.parse import urljoin\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup, Tag\n",
    "# from dateutil import parser as dateparser\n",
    "# from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34510d",
   "metadata": {},
   "source": [
    "### Class to scrape the XML links and to collate the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6eed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import annotations\n",
    "\n",
    "# import asyncio\n",
    "# import re\n",
    "# from dataclasses import dataclass\n",
    "# from datetime import datetime, timezone\n",
    "# from pathlib import Path\n",
    "# from typing import Iterable, List\n",
    "# import httpx\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# MONTH_FEED_RE = re.compile(r\"/(\\d{4})/(\\d{2})/feeds\\.xml$\")   # keep only YYYY/MM feeds\n",
    "\n",
    "# @dataclass\n",
    "# class LinkScraper:\n",
    "#     index_url: str\n",
    "#     out_dir: Path | str = \"/home/leeeefun681/volume/eefun/webscraping/sitemap/sitemap_scrape/data/straitsTimes/st_sitemaps\"\n",
    "#     timeout: float = 15.0\n",
    "#     polite_delay: float = 1.0\n",
    "#     max_concurrency: int = 5\n",
    "\n",
    "#     # ────────────────────────── public helpers ──────────────────────────────\n",
    "#     async def dump_async(self) -> None:\n",
    "#         \"\"\"Asynchronously download every past month and save to .txt files.\"\"\"\n",
    "#         self.out_dir = Path(self.out_dir)\n",
    "#         self.out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#         async with httpx.AsyncClient(timeout=self.timeout) as client:\n",
    "#             month_feeds = await self._sitemap_links(client)\n",
    "\n",
    "#             if not month_feeds:\n",
    "#                 print(\"No month feeds found (or all filtered out).\")\n",
    "#                 return\n",
    "\n",
    "#             sem = asyncio.Semaphore(self.max_concurrency)\n",
    "#             tasks = [\n",
    "#                 asyncio.create_task(self._process_month(feed_url, client, sem))\n",
    "#                 for feed_url in month_feeds\n",
    "#             ]\n",
    "#             await asyncio.gather(*tasks)\n",
    "\n",
    "#     def dump(self):\n",
    "#         try:\n",
    "#             loop = asyncio.get_running_loop()\n",
    "#         except RuntimeError:           # no loop → we're in a vanilla script\n",
    "#             loop = None\n",
    "\n",
    "#         if loop and loop.is_running():\n",
    "#             # notebook / web-server context → create and return a Task\n",
    "#             return asyncio.create_task(self.dump_async())\n",
    "#         else:\n",
    "#             # classic script → safe to spin up a fresh loop\n",
    "#             asyncio.run(self.dump_async())\n",
    "\n",
    "#     # ────────────────────────── internals ───────────────────────────────────\n",
    "#     async def _sitemap_links(self, client: httpx.AsyncClient) -> List[str]:\n",
    "#         \"\"\"Return monthly feeds, filtering out current month & sections.xml.\"\"\"\n",
    "#         r = await client.get(self.index_url)\n",
    "#         r.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(r.content, \"xml\")\n",
    "#         raw_links = [\n",
    "#             loc.get_text(strip=True)\n",
    "#             for loc in soup.find_all(\"loc\")\n",
    "#             if loc.parent.name == \"sitemap\"\n",
    "#         ]\n",
    "\n",
    "#         # figure out YYYY/MM for 'today' (Singapore time is irrelevant for month test)\n",
    "#         y_now, m_now = datetime.now(timezone.utc).year, datetime.now(timezone.utc).month\n",
    "\n",
    "#         feeds: list[str] = []\n",
    "#         for link in raw_links:\n",
    "#             m = MONTH_FEED_RE.search(link)\n",
    "#             if not m:                          # skips sections.xml & anything odd\n",
    "#                 continue\n",
    "#             yr, mo = int(m.group(1)), int(m.group(2))\n",
    "#             if (yr, mo) == (y_now, m_now):     # skip current month\n",
    "#                 continue\n",
    "#             feeds.append(link)\n",
    "\n",
    "#         return feeds\n",
    "\n",
    "#     async def _month_urls(\n",
    "#         self, feed_url: str, client: httpx.AsyncClient\n",
    "#     ) -> Iterable[str]:\n",
    "#         \"\"\"Return every <loc> article URL from a single feeds.xml.\"\"\"\n",
    "#         r = await client.get(feed_url)\n",
    "#         r.raise_for_status()\n",
    "#         soup = BeautifulSoup(r.content, \"xml\")\n",
    "#         return (loc.get_text(strip=True) for loc in soup.find_all(\"loc\"))\n",
    "\n",
    "#     async def _process_month(\n",
    "#         self,\n",
    "#         feed_url: str,\n",
    "#         client: httpx.AsyncClient,\n",
    "#         sem: asyncio.Semaphore,\n",
    "#     ) -> None:\n",
    "#         \"\"\"Download one month feed, write out its TXT file.\"\"\"\n",
    "#         async with sem:                 # limit concurrent requests\n",
    "#             try:\n",
    "#                 urls = list(await self._month_urls(feed_url, client))\n",
    "#             except httpx.HTTPError as e:\n",
    "#                 print(\"   ERR ·\", feed_url, \"→\", e)\n",
    "#                 return\n",
    "\n",
    "#             if not urls:\n",
    "#                 print(\"   0   · (empty) ·\", feed_url)\n",
    "#                 return\n",
    "\n",
    "#             # derive filename st_YYYY_MM.txt from the URL\n",
    "#             m = MONTH_FEED_RE.search(feed_url)\n",
    "#             fname = f\"st_{m.group(1)}_{m.group(2)}.txt\"\n",
    "#             outpath = self.out_dir / fname\n",
    "#             outpath.write_text(\"\\n\".join(urls), encoding=\"utf-8\")\n",
    "\n",
    "#             print(f\"{len(urls):5d} · {outpath.relative_to(self.out_dir)}\")\n",
    "#             await asyncio.sleep(self.polite_delay)  # respectful crawl pace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5287333e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor = LinkScraper(\n",
    "#     index_url=\"https://www.straitstimes.com/sitemap.xml\",\n",
    "#     timeout=10,\n",
    "#     polite_delay=0.5,\n",
    "#     max_concurrency=8,\n",
    "# )\n",
    "# extractor.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3c86b",
   "metadata": {},
   "source": [
    "<!-- ### Class to scrape 1 singular article -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e349a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from dateutil import parser as dateparser\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "\n",
    "class ArticleScraper:\n",
    "    #   Scrapes a page for <article> → images + captions.\n",
    "    #   Each image is returned as its own JSON object with keys: site_title, publish_date, image_url, alt_text, caption\n",
    "\n",
    "    def _extract_image_src(self, img_tag: Tag, page_url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Resolve relative/absolute URLs for the given <img>.\n",
    "        \"\"\"\n",
    "        src = img_tag.get(\"src\") or img_tag.get(\"data-src\") or img_tag.get(\"data-original\")\n",
    "        return urljoin(page_url, src) if src else None\n",
    "\n",
    "    def _extract_caption(self, img_tag: Tag) -> Optional[str]:\n",
    "        # Return the best-guess caption for `img_tag`.\n",
    "\n",
    "        # 1. Ideal case: <figure> → <figcaption>\n",
    "        fig = img_tag.find_parent(\"figure\")\n",
    "        if fig:\n",
    "            figcap = fig.find(\"figcaption\")\n",
    "            if figcap:\n",
    "                text = figcap.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 2. Immediate siblings (<figcaption>, <p>, <span>)\n",
    "        for sib in (img_tag.find_next_sibling(), img_tag.find_previous_sibling()):\n",
    "            if sib and isinstance(sib, Tag) and sib.name in {\"figcaption\", \"p\", \"span\"}:\n",
    "                text = sib.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "\n",
    "        # 3. Any ancestor with caption-like class\n",
    "        parent = img_tag.parent\n",
    "        while parent and parent.name not in {\"article\", \"body\"}:\n",
    "            classes = parent.get(\"class\", [])\n",
    "            if any(re.search(r\"(caption|credit)\", c, re.I) for c in classes):\n",
    "                text = parent.get_text(\" \", strip=True)\n",
    "                if text:\n",
    "                    return text\n",
    "            parent = parent.parent\n",
    "\n",
    "        # 4. Up to 3 forward/back block-level siblings\n",
    "        for direction in (\"next\", \"previous\"):\n",
    "            sib_iter = (\n",
    "                img_tag.next_siblings if direction == \"next\" else img_tag.previous_siblings\n",
    "            )\n",
    "            count = 0\n",
    "            for sib in sib_iter:\n",
    "                if isinstance(sib, Tag) and sib.name in {\"p\", \"div\", \"figcaption\", \"span\"}:\n",
    "                    text = sib.get_text(\" \", strip=True)\n",
    "                    if text:\n",
    "                        return text\n",
    "                    count += 1\n",
    "                    if count == 3:  # stop after 3 hops\n",
    "                        break\n",
    "\n",
    "        # 5. Last resort: alt text\n",
    "        alt = img_tag.get(\"alt\", \"\").strip()\n",
    "        return alt or None\n",
    "\n",
    "\n",
    "    def scrape(self, url: str) -> List[Dict[str, Any]]:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "        article = soup.find(\"article\")\n",
    "        if not article:\n",
    "            raise RuntimeError(\"No <article> tag found\")\n",
    "\n",
    "        # Title\n",
    "        h1 = article.find(\"h1\")\n",
    "        title = (\n",
    "            h1.get_text(strip=True)\n",
    "            if h1\n",
    "            else (soup.title.string.strip() if soup.title else \"(untitled)\")\n",
    "        )\n",
    "\n",
    "        # Published date\n",
    "        pub_date: Optional[str] = None\n",
    "        time_tag = article.find(\"time\")\n",
    "        if time_tag and time_tag.has_attr(\"datetime\"):\n",
    "            pub_date = dateparser.parse(time_tag[\"datetime\"]).isoformat()\n",
    "        elif time_tag:\n",
    "            pub_date = dateparser.parse(time_tag.get_text(strip=True)).isoformat()\n",
    "        else:\n",
    "            meta = soup.find(\"meta\", {\"property\": \"article:published_time\"})\n",
    "            if meta and meta.has_attr(\"content\"):\n",
    "                pub_date = dateparser.parse(meta[\"content\"]).isoformat()\n",
    "\n",
    "        # Collect images\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for img in article.find_all(\"img\"):\n",
    "            src = self._extract_image_src(img, url)\n",
    "            if not src:\n",
    "                continue\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"site_title\": title,\n",
    "                    \"publish_date\": pub_date,\n",
    "                    \"image_url\": src,\n",
    "                    \"alt_text\": img.get(\"alt\", \"\").strip() or None,\n",
    "                    \"caption\": self._extract_caption(img),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79e423d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe0899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b605be67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db66c103422643f2b2625ad121a20729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/tmp/ipykernel_2451429/3393277453.py\", line 17, in process_txt\n    out_file   = OUT_DIR / f\"{year_month}.jsonl\"\nTypeError: unsupported operand type(s) for /: 'NoneType' and 'str'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m txt_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(TXT_DIR\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mst_*.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mProcessPoolExecutor(MAX_FILES_PARALLEL) \u001b[38;5;28;01mas\u001b[39;00m outer_pool:\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(outer_pool\u001b[38;5;241m.\u001b[39mmap(process_txt, txt_files),\n\u001b[1;32m      4\u001b[0m                   total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(txt_files), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ All done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/intern_volume/intern_volume/eefun/webscraping/sitemap/sitemap_scrape/.venv/lib/python3.10/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/home/intern_volume/intern_volume/eefun/webscraping/sitemap/sitemap_scrape/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "# txt_files = sorted(TXT_DIR.glob(\"st_*.txt\"))\n",
    "# with concurrent.futures.ProcessPoolExecutor(MAX_FILES_PARALLEL) as outer_pool:\n",
    "#     for _ in tqdm(outer_pool.map(process_txt, txt_files), total=len(txt_files), desc=\"files\"):\n",
    "#         pass\n",
    "# print(\"✅ All done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
